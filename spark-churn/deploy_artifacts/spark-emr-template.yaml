AWSTemplateFormatVersion: '2010-09-09'
Description: Provision an EMR cluster with Spark 
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
    - Label:
        default: EMR Options
      Parameters:
        - emrClusterName
        - InstanceCount
        - InstanceType
        - ReleaseLabel
        #- emrKeyName
        #- emrLogUri
        - VPCId
        - VPCSubnet
        - LivyPort
        - emrBootstrapScript
        - NameTag
        - EnvironmentName
Parameters:
  #emrLogUri:
  #  Type: String
  #  Default: s3://<bucket>/emrlogs/  
  ReleaseLabel:
    Type: String
    Description: EMR release to use
    Default: emr-5.15.0
  emrClusterName:
    Type: String
    Default: sageemr
  EnvironmentName:
    Type: String
    Description: >
      An environment name that will be prefixed to resource names including
      exported values. Should be unique per region.
    Default: sparksage
  #emrKeyName:
  #  Description: SSH key pair to use for EMR node login
  #  Type: AWS::EC2::KeyPair::KeyName
  #  Default: winkey
  VPCId:
    Description: VPC for EMR nodes.
    Type: AWS::EC2::VPC::Id
  VPCSubnet:
    Description: Subnet for EMR nodes, from the VPC selected above
    Type: AWS::EC2::Subnet::Id
  InstanceCount:
    Description: Number of core nodes to provision (1-20)
    Type: Number
    MinValue: '1'
    MaxValue: '20'
    Default: '2'
  LivyPort:
    Type: Number
    Default: 8998
    Description: Port for Livy service to listen on      
  InstanceType:
    Type: String
    Default: m4.2xlarge
    AllowedValues:
    - m4.large
    - m4.xlarge
    - m4.2xlarge
    - m4.4xlarge
    - m5.xlarge
    - m5.2xlarge
    - m5.4xlarge
    - c4.large
    - c4.xlarge
    - c4.2xlarge
    - c4.4xlarge
    - c5.xlarge
    - c5.2xlarge
    - c5.4xlarge
    Description: EMR node ec2 instance type - you can add more types by expandingon this list.
  NameTag:
    Type: String
    MinLength: 1
    Default: sparksage
    Description: Environment name of the cluster
  SageMakerInstanceSecurityGroup:
    Description: Security group that the SageMaker instance will launched in
    #Type: AWS::EC2::SecurityGroup
    Type: String
Resources:
  AllowLivyPort:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: sparksage
      VpcId:
        Ref: VPCId
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: !Ref LivyPort
        ToPort: !Ref LivyPort
        SourceSecurityGroupId: !Ref SageMakerInstanceSecurityGroup
  rEMREC2InstanceProfile:
    Properties:
      Path: "/"
      Roles:
      - Ref: rEMREC2Role
    Type: AWS::IAM::InstanceProfile
  rEMREC2Role:
    Type: AWS::IAM::Role
    Properties:
      Path: "/"
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - ec2.amazonaws.com
        Version: '2012-10-17'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role
      Policies:
      - PolicyName: EMRAccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Resource: "*"
            Action:
            - elasticmapreduce:list*
            #- athena:*
            #- glue:*
            Effect: Allow
  rEMRServiceRole:
    Type: AWS::IAM::Role
    Properties:
      Path: "/"
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - elasticmapreduce.amazonaws.com
        Version: '2012-10-17'
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole
  SparkCluster:
    DependsOn: EMRCleanup
    Type: AWS::EMR::Cluster
    Properties:
      Applications:
      - Name: Hadoop
      - Name: Hive
      - Name: Spark
      - Name: Ganglia
      - Name: Livy
      BootstrapActions:
      - Name: Install-Packages
        ScriptBootstrapAction:
          Path: file:/usr/bin/sudo
          Args:
          - "pip"
          - "install"
          - "boto3"
          - "pandas"
          - "sklearn"  
          #- "pyathenajdbc"
      Configurations:
      - Classification: hadoop-log4j
        ConfigurationProperties:
          log4j.rootCategory: 'ERROR,console'
        Configurations: []
      - Classification: spark
        ConfigurationProperties:
          maximizeResourceAllocation: true
        Configurations: []
      - Classification: spark-log4j
        ConfigurationProperties:
          log4j.logger.org.apache.spark: ERROR
        Configurations: []        
      - Classification: livy-conf
        ConfigurationProperties:
          livy.repl.enable-hive-context: true
        Configurations: []  
      - Classification: hive-site
        ConfigurationProperties:
          hive.metastore.client.factory.class: com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory
        Configurations: []  
      - Classification: spark-hive-site
        ConfigurationProperties:
          hive.metastore.client.factory.class: com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory
        Configurations: []
      Instances:
        AdditionalMasterSecurityGroups:
        - Fn::GetAtt:
          - AllowLivyPort
          - GroupId
        #Ec2KeyName:
        #  Ref: emrKeyName
        Ec2SubnetId:
          Ref: VPCSubnet
        MasterInstanceGroup:
          InstanceCount: 1
          InstanceType:
            Ref: InstanceType
        CoreInstanceGroup:
          InstanceCount:
            Ref: InstanceCount
          InstanceType:
            Ref: InstanceType
      Name: !Ref emrClusterName
      JobFlowRole:
        Ref: rEMREC2InstanceProfile
      ServiceRole:
        Ref: rEMRServiceRole
      VisibleToAllUsers: true
      ReleaseLabel: !Ref ReleaseLabel
      #LogUri: !Ref emrLogUri
      Tags:
      - Key: Name
        Value: !Sub "${NameTag}-spark-sage"
  EMRCleanup:
    Type: Custom::EMRCleanup
    Properties:
      ServiceToken: !GetAtt EMRCleanupFunction.Arn
  EMRCleanupFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt EMRCleanupExecutionRole.Arn
      Runtime: python3.7
      Timeout: 300
      Code:
        ZipFile: !Sub |
            from __future__ import print_function
            import json
            import boto3
            import cfnresponse
            import time
            def handler(event, context):
                print(json.dumps(event))
                if (event["RequestType"] == "Delete"):
                    try:
                        deleteSecurityGroups("${VPCId}")
                    except Exception as e:
                        print("Exception thrown: %s" % str(e))
                        pass
                else:
                    print("RequestType %s, nothing to do" % event["RequestType"])
                time.sleep(30)  # pause for CloudWatch logs
                print('Done')
                responseData={"Data":"OK"}
                cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
            def deleteSecurityGroups(vpcid):
                time.sleep(30)  # delay to avoid dependency race condition
                ec2 = boto3.resource('ec2')
                vpc = ec2.Vpc(vpcid)
                # Fist, delete EMR Default VPC Security Group Rules
                for sg in vpc.security_groups.all():
                   if "ElasticMapReduce" not in sg.group_name:
                       continue
                   print("Deleting rules for SG: " + str(sg))
                   for rule in sg.ip_permissions:
                       try:
                           sg.revoke_ingress(
                               IpPermissions=[{
                                   "IpProtocol":rule["IpProtocol"],
                                   "FromPort":rule["FromPort"],
                                   "ToPort":rule["ToPort"],
                                   "UserIdGroupPairs":rule["UserIdGroupPairs"]}]
                               )
                       except Exception as e:
                           print(str(e))
                # Now, delete the VPC Security Groups
                for sg in vpc.security_groups.all():
                   if "ElasticMapReduce" not in sg.group_name:
                       continue
                   print("Deleting SG: " + str(sg))
                   try:
                       sg.delete()
                   except Exception as e:
                       print(str(e))
                       pass
  EMRCleanupExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: "/"
      Policies:
      - PolicyName: LogsForLambda
        PolicyDocument:
          Version: 2012-10-17
          Statement:
            - Effect: Allow
              Action:
                - logs:CreateLogGroup
                - logs:CreateLogStream
                - logs:PutLogEvents
              Resource:
                - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${AWS::StackName}*"
                - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${AWS::StackName}*:*"
      - PolicyName: EC2DescribeDeleleRevokeSg
        PolicyDocument:
          Version: 2012-10-17
          Statement:
            - Effect: Allow
              Action:
                - ec2:Describe*
                - ec2:DeleteSecurityGroup
                - ec2:RevokeSecurityGroupIngress
              Resource: '*'
              Condition:
                ArnEqualsIfExists:
                  ec2:Vpc: !Sub "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:vpc/${VPCId}"

Outputs:
  EMRClusterId:
    Value: !Ref SparkCluster
