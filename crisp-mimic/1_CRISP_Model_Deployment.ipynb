{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying CRISP-DM on the MIMIC-3 Dataset (Part II)\n",
    "In this notebook, I apply the 3 last phases of the CRISP-DM process:\n",
    "\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment\n",
    "\n",
    "<img src=\"./media/crisp.png\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note here is that we prepare the data for training the model (it would be still related to the step 3). However, the main part of this notebook is related to the final 3 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import awswrangler as wr\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The small part related to step 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'mimiciii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "with ce as\n",
    "(\n",
    "  select\n",
    "    icustay_id, charttime, itemid, valuenum\n",
    "  from chartevents\n",
    "  -- specify what data we want from chartevents\n",
    "  where itemid in\n",
    "  (\n",
    "  211, -- Heart Rate\n",
    "  618, --\tRespiratory Rate\n",
    "  615 --\tResp Rate (Total)\n",
    "  )\n",
    "  -- how did we know heart rates were stored using ITEMID 211? Simple, we looked in D_ITEMS!\n",
    "  -- Try it for yourself: select * from d_items where lower(label) like '%heart rate%'\n",
    ")\n",
    "select\n",
    "  -- ICUSTAY_ID identifies each unique patient ICU stay\n",
    "  -- note that if the same person stays in the ICU more than once, each stay would have a *different* ICUSTAY_ID\n",
    "  -- however, since it's the same person, all those stays would have the same SUBJECT_ID\n",
    "  ie.icustay_id\n",
    "\n",
    "  -- this is the outcome of interest: in-hospital mortality\n",
    "  , max(adm.HOSPITAL_EXPIRE_FLAG) as OUTCOME\n",
    "\n",
    "  -- this is a case statement - essentially an \"if, else\" clause\n",
    "  , min(\n",
    "      case\n",
    "        -- if the itemid is 211\n",
    "        when itemid = 211\n",
    "          -- then return the actual value stored in VALUENUM\n",
    "          then valuenum\n",
    "        -- otherwise, return 'null', which is SQL standard for an empty value\n",
    "        else null\n",
    "      -- end the case statement\n",
    "      end\n",
    "    ) as HeartRate_Min\n",
    "\n",
    "    -- note we wrapped the above in \"min()\"\n",
    "    -- this takes the minimum of all values inside, and *ignores* nulls\n",
    "    -- by calling this on our case statement, we are ignoring all values except those with ITEMID = 211\n",
    "    -- since ITEMID 211 are heart rates, we take the minimum of only heart rates\n",
    "\n",
    "  , max(case when itemid = 211 then valuenum else null end) as HeartRate_Max\n",
    "  , min(case when itemid in (615,618) then valuenum else null end) as RespRate_Min\n",
    "  , max(case when itemid in (615,618) then valuenum else null end) as RespRate_Max\n",
    "from icustays ie\n",
    "\n",
    "-- join to the admissions table to get hospital outcome\n",
    "inner join admissions adm\n",
    "  on ie.hadm_id = adm.hadm_id\n",
    "\n",
    "-- join to the chartevents table to get the observations\n",
    "left join ce\n",
    "  -- match the tables on the patient identifier\n",
    "  on ie.icustay_id = ce.icustay_id\n",
    "  -- and require that the observation be made after the patient is admitted to the ICU\n",
    "  and ce.charttime >= ie.intime\n",
    "  -- and *before* their admission time + 1 day, i.e. the observation must be made on their first day in the ICU\n",
    "  and ce.charttime <= ie.intime + interval '1' day\n",
    "group by ie.icustay_id\n",
    "order by ie.icustay_id\n",
    "\"\"\"\n",
    "\n",
    "data = wr.athena.read_sql_query(query,database)\n",
    "data.drop('icustay_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>heartrate_min</th>\n",
       "      <th>heartrate_max</th>\n",
       "      <th>resprate_min</th>\n",
       "      <th>resprate_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outcome  heartrate_min  heartrate_max  resprate_min  resprate_max\n",
       "0        0            NaN            NaN           NaN           NaN\n",
       "1        0           72.0          122.0          14.0          39.0\n",
       "2        0           62.0           84.0          14.0          27.0\n",
       "3        0           80.0          104.0          13.0          29.0\n",
       "4        0           88.0          106.0          13.0          22.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61532, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We should have +60K rows in our dataset represeting patients from the MIMIC-III dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "\n",
    "Here we develop, train and test the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move from a data frame into a numpy array\n",
    "X = data.iloc[:,1:].to_numpy()\n",
    "y = data['outcome'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to numeric types\n",
    "X = X.astype(float)\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in 80%-20% training/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute mean for missing values, since we have only numerical values\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(fit_intercept=True, solver='lbfgs')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "We evaluate our model locally, analyzing the test dataset and using AUROC, confusion matrices and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8956691313886406\n",
      "AUROC = 0.6181023509772555\n",
      "\n",
      "Confusion matrix\n",
      "[[10979    21]\n",
      " [ 1263    44]]\n",
      "\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.94     11000\n",
      "           1       0.68      0.03      0.06      1307\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     12307\n",
      "   macro avg       0.79      0.52      0.50     12307\n",
      "weighted avg       0.87      0.90      0.85     12307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict class labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# generate class probabilities\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "# generate evaluation metrics\n",
    "print('Accuracy = {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('AUROC = {}'.format(roc_auc_score(y_test, y_prob[:, 1])))\n",
    "\n",
    "print('\\nConfusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nClassification report')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC for all folds:\n",
      "[0.61605425 0.64243601 0.62164081 0.60277423 0.61229702]\n",
      "Average AUROC across folds:\n",
      "0.6190404642649556\n"
     ]
    }
   ],
   "source": [
    "# evaluate a logistic regression with 5-fold cross-validation\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values=np.nan,\n",
    "                                          strategy=\"mean\")),\n",
    "                      ('scaler', StandardScaler()),\n",
    "                      (\"regression\", LogisticRegressionCV(cv=5,\n",
    "                                                          scoring='roc_auc',\n",
    "                                                          solver='lbfgs'))])\n",
    "\n",
    "scores = cross_val_score(estimator\n",
    "                         , X, y\n",
    "                         , scoring='roc_auc', cv=5)\n",
    "\n",
    "\n",
    "print('AUROC for all folds:')\n",
    "print(scores)\n",
    "print('Average AUROC across folds:')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8956691313886406\n",
      "AUROC = 0.6181023509772555\n"
     ]
    }
   ],
   "source": [
    "# train pipeline\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# predict class labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# generate class probabilities\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "# generate evaluation metrics\n",
    "print('Accuracy = {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('AUROC = {}'.format(roc_auc_score(y_test, y_prob[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test saving and loading model locally\n",
    "from sklearn.externals import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(estimator, os.path.join('./', \"model.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "       verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('regression', LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, i...andom_state=None, refit=True, scoring='roc_auc',\n",
       "           solver='lbfgs', tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = joblib.load(os.path.join('./', \"model.joblib\"))\n",
    "\n",
    "estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment\n",
    "We finally deploy in SageMaker, in a managed training and hosting infrastructure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about SageMaker, get out the [docs](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html).\n",
    "\n",
    "Remember to add permissions of SageMakerFullAccess to IAM Role of the notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'Scikit-mimic'\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory and write csv\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "np.savetxt('./data/mimic.csv', data, delimiter=',', fmt='%1.1f, %1.3f, %1.3f, %1.3f, %1.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in S3 for training with SageMaker\n",
    "data_dir = 'data'\n",
    "train_input = sagemaker_session.upload_data(data_dir, key_prefix=\"{}/{}\".format(prefix, data_dir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = 'scikit_learn_mimic.py'\n",
    "\n",
    "# Simple training without hyperparameters\n",
    "# The entry script in a python script based on the code developed above\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.c4.xlarge\", #\"local\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-08 16:51:22 Starting - Starting the training job...\n",
      "2020-07-08 16:51:24 Starting - Launching requested ML instances......\n",
      "2020-07-08 16:52:47 Starting - Preparing the instances for training......\n",
      "2020-07-08 16:53:50 Downloading - Downloading input data...\n",
      "2020-07-08 16:54:19 Training - Downloading the training image...\n",
      "2020-07-08 16:54:40 Training - Training image download completed. Training in progress.\u001b[34m2020-07-08 16:54:40,851 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-07-08 16:54:40,854 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-08 16:54:40,874 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-07-08 16:54:56,510 sagemaker-containers INFO     Module scikit_learn_mimic does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-07-08 16:54:56,511 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-07-08 16:54:56,511 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-07-08 16:54:56,511 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: scikit-learn-mimic\n",
      "  Building wheel for scikit-learn-mimic (setup.py): started\n",
      "  Building wheel for scikit-learn-mimic (setup.py): finished with status 'done'\n",
      "  Created wheel for scikit-learn-mimic: filename=scikit_learn_mimic-1.0.0-py2.py3-none-any.whl size=8556 sha256=e6d21568b1b1edbbddc7205a9766045706c459e6c8f05493254382fceca6a9c7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-czd505f9/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built scikit-learn-mimic\u001b[0m\n",
      "\u001b[34mInstalling collected packages: scikit-learn-mimic\u001b[0m\n",
      "\u001b[34mSuccessfully installed scikit-learn-mimic-1.0.0\u001b[0m\n",
      "\u001b[34m2020-07-08 16:54:57,888 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-08 16:54:57,899 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2020-07-08-16-51-22-118\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-795875386142/sagemaker-scikit-learn-2020-07-08-16-51-22-118/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"scikit_learn_mimic\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"scikit_learn_mimic.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=scikit_learn_mimic.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=scikit_learn_mimic\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-795875386142/sagemaker-scikit-learn-2020-07-08-16-51-22-118/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2020-07-08-16-51-22-118\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-795875386142/sagemaker-scikit-learn-2020-07-08-16-51-22-118/source/sourcedir.tar.gz\",\"module_name\":\"scikit_learn_mimic\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"scikit_learn_mimic.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m scikit_learn_mimic\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mAccuracy = 0.8893312748842123\u001b[0m\n",
      "\u001b[34mAUROC = 0.6251032622805197\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34m2020-07-08 16:55:02,148 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-07-08 16:55:13 Uploading - Uploading generated training model\n",
      "2020-07-08 16:55:13 Completed - Training job completed\n",
      "Training seconds: 83\n",
      "Billable seconds: 83\n"
     ]
    }
   ],
   "source": [
    "sklearn.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see SageMaker spinning up instances for training:\n",
    "\n",
    "![sm-train](./media/sagemaker-training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see SageMaker spinning up instances for hosting the model with an API endpoint:\n",
    "\n",
    "![sm-train](./media/sagemaker-hosting.png)\n",
    "\n",
    "![sm-train](./media/sagemaker-hosting-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8956691313886406\n",
      "AUROC = 0.6181023509772555\n",
      "\n",
      "Confusion matrix\n",
      "[[10979    21]\n",
      " [ 1263    44]]\n",
      "\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.94     11000\n",
      "           1       0.68      0.03      0.06      1307\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     12307\n",
      "   macro avg       0.79      0.52      0.50     12307\n",
      "weighted avg       0.87      0.90      0.85     12307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict class labels for the test set\n",
    "y_pred_endpoint = predictor.predict(X_test)\n",
    "\n",
    "# generate evaluation metrics\n",
    "print('Accuracy = {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('AUROC = {}'.format(roc_auc_score(y_test, y_prob[:, 1])))\n",
    "\n",
    "print('\\nConfusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nClassification report')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint for avoid costs\n",
    "sklearn.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
